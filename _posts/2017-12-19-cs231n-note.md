---
layout: post
title: CS231n note
data: 2017-12-19
author: Nicholas Huang
header-img: 
categories: DeepLearning
tags:
    - Deep Learning
--- 
# CS231n note
我自己记录的关于斯坦福CS231n的课程笔记，[CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/)
## classification
有几个挑战：

1. 视角变化
2. 尺度变化
3. 形变
4. 遮挡
5. 光照条件
6. 背景噪声
7. 同类变化

一个好的图像分类模型必须对这些变化的交集的识别保持统一，同时对异类变化保持敏感。
>A good image classification model must be invariant to the cross product of all these variations, while simultaneously retaining sensitivity to the inter-class variations.

其实上面的话就是要求模型具有良好的泛化性，既不能欠拟合——要对各种变化的交叉再组合要能正确的识别，也不能过拟合——对其他类型能正确识别。

不能把测试集用于调试超参数，应该从训练集中分离出验证集，用于调试超参数。
>we cannot use the test set for the purpose of tweaking hyperparameters.
>Split your training set into training set and a validation set. Use validation set to tune all hyperparameters. At the end run a single time on the test set and report performance.

通过对L1和L2的Nearest Neighbor classifier的讲解，说明了专注于像素是不行的。
>In particular, note that images that are nearby each other are much more a function of the general color distribution of the images, or the type of background rather than their semantic identity. For example, a dog can be seen very near a frog since both happen to be on white background. Ideally we would like images of all of the 10 classes to form their own clusters, so that images of the same class are nearby to each other regardless of irrelevant characteristics and variations (such as the background). However, to get this property we will have to go beyond raw pixels.

## optimization-2
对后向传播(backpropagation)的定义：
>a way of computing gradients of expressions through recursive application of chain rule.

对于函数$f(x,y) = xy$来说，它的梯度$\nabla f$是$[\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}]$

把前向传递(forward pass)分解成几个简单表达式，每个表达式对应一个中间变量，有利于简化后向传递(backward pass)的实现——后向传递可以对前向传递中的每个中间变量求导，通过乘法将它们链接起来。

加法门(add gate)在输出处获得梯度并分发到两个输入处。因为加法本身的梯度是1.0，所以输入处的梯度等于输出处的梯度乘以1.0。
最大门(max gate)会把输出处的梯度路由到它选用的那个输入处——即最大值所在的地方。因为max对最大值的梯度是1.0，其他值是0.0。
乘法门(multiply gate)的每一个输入处梯度就是另一个乘数乘以输出处的梯度。因此，乘法门会有一些不直观的效果：
**1.**如果两个乘数一个大一个小，会造成每个乘数的梯度差别很大。
**2.**还有就是某个乘数很大的话会造成另一个乘数的梯度下降太快，此时需要对learning rate做调整。

