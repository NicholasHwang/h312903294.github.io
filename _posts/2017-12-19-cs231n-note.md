# CS231n note
我自己记录的关于斯坦福CS231n的课程笔记，[CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/)
##optimization-2
对后向传播(backpropagation)的定义：
>a way of computing gradients of expressions through recursive application of chain rule.

对于函数$f(x,y) = xy$来说，它的梯度$\nabla f$是$[\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}]$

把前向传递(forward pass)分解成几个简单表达式，每个表达式对应一个中间变量，有利于简化后向传递(backward pass)的实现——后向传递可以对前向传递中的每个中间变量求导，通过乘法将它们链接起来。

加法门(add gate)在输出处获得梯度并分发到两个输入处。因为加法本身的梯度是1.0，所以输入处的梯度等于输出处的梯度乘以1.0。
最大门(max gate)会把输出处的梯度路由到它选用的那个输入处——即最大值所在的地方。因为max对最大值的梯度是1.0，其他值是0.0。
乘法门(multiply gate)的每一个输入处梯度就是另一个乘数乘以输出处的梯度。因此，乘法门会有一些不直观的效果：
**1.**如果两个乘数一个大一个小，会造成每个乘数的梯度差别很大。
**2.**还有就是某个乘数很大的话会造成另一个乘数的梯度下降太快，此时需要对learning rate做调整。

