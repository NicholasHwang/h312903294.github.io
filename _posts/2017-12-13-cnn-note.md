---
layout: post
title: CNN流程简单梳理
data: 2017-12-13
author: Nicholas Huang
header-img: 
categories: CNN
tags:
    - CNN
    - Deep Learning
--- 
# CNN流程简单梳理
我理解的CNN就是一个模拟人脑认知的网络，通过filter（或者叫kernel）识别数据特征——类比人眼对数据生成电信号，对特征利用各种数学运算以完成区分——模拟人脑中的神经元对电信号的反应，多次重复这个过程以达到识别、认知的目的。

<!-- CNN整体图 -->
先用几张图来比较形象的观察下CNN的流程：

![CNN1](https://www.mathworks.com/content/mathworks/www/en/discovery/convolutional-neural-network/jcr:content/mainParsys/image_copy.adapt.full.high.jpg/1492406018870.jpg)

该图来源于[pic source1](https://www.mathworks.com/content/mathworks/www/en/discovery/convolutional-neural-network/jcr:content/mainParsys/image_copy.adapt.full.high.jpg/1492406018870.jpg)

在这张图上，可以看到左边的一张车的图片输入到CNN后，右边会出来多个识别结果，比如小汽车(car)，卡车(truck)，面包车(van)，自行车(bicycle)等，每个结果会对应一个0到1的概率，概率值越大表示属于这个结果的可能性越大。

![CNN2](https://www.researchgate.net/profile/B_Mesman/publication/220785200/figure/fig1/AS:340720692023305@1458245551937/Fig-1-An-Example-CNN-architecture-for-a-handwritten-digit-recognition-task.png)

该图来源于[pic source2](https://www.researchgate.net/profile/B_Mesman/publication/220785200/figure/fig1/AS:340720692023305@1458245551937/Fig-1-An-Example-CNN-architecture-for-a-handwritten-digit-recognition-task.png)

在这张图上，形象的表示了特征图(feature map)。

通过这两张图，我们能够看出共性的东西：
**1.** 都有卷积层(convolution layer)，并且卷积层会有多个，卷积层包含两个操作，卷积(convolution)和ReLU。
**2.** 都有池化层(pooling layer or subsampling layer)，它的输入就是卷积层的输出。但是不能认为每一个卷积层后面就会有一个池化层，可以多个卷积层后接一个池化层。
**3.** 都有全连接层(fully connected layer)，它就是把前面计算出来的结果两两相乘。需要注意的是，在第一个全连接层的输入处有一个拉平(flatten)的操作，它是把在这层之前的池化层的结果拉平成一个[1, N_Classes]，其中N_Classes就是需要识别的标签的个数。

看过了CNN的内部结构，相当于认识了CNN的“硬件”是怎样构成的，现在我们需要了解下CNN的“软件”是怎样运作的。推荐[A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)的Training小节。一句话总结就是，CNN的更新权值(weight)的过程叫后向传播(backpropagation)。后向传播由前向传递(forward pass)、损失函数(loss function)、后向传递(backward pass)和权值更新(weight update)四部分组成，[Principles of training multi-layer neural network using backpropagation](http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html)用图表示了整个过程。后向传播算法的原理请看[《deep learning》](http://www.deeplearningbook.org/)6.5节。后向传播算法主要是用于计算梯度，而另一种算法，比如随机梯度下降(SGD)、Adam，用该梯度来进行学习。

以下均用Google的tensorflow框架做代码演示。
## 前向传递(forward pass)
### 卷积层（Convolutioanl Layer)
#### 卷积运算
##### 什么是卷积
卷积的[wiki](https://en.wikipedia.org/wiki/Convolution)，我找到的一个比较好的说明[知乎](https://www.zhihu.com/question/22298352)。我个人自己总结的话，就是对两个同维矩阵做点积之后的新矩阵中的每个值求和。每个和就是一个特征（feature），这也就是过滤的过程。

其实这种通过提取特征而识别整体的过程，古代就已经出现了，而且我们还学习了相关课文的，即盲人摸象。
每个盲人只感知了象的一部分，把他们所有的意见合并起来，就形成了一个完整的象。
如果上面这个举例还不够清楚的话，那我再举个例子。
假如我们在一个绝对黑暗的房间，我们在这个房间地面的中心，右手有一个手电筒，面对着一个有蒙娜丽莎画面的墙壁。当我们举起右手的手电筒，从上往下，从左至右，依次照射，那么从每一次的照射中就看到了一小块区域，当手电筒照到右下角的时候，整个过程完毕，而我们也形成了对蒙娜丽莎的整体认知。

卷积运算的核心是filter，又称kernel。根据名字我们能够猜测出，这个核心就是一个过滤器，他把特征过滤出来，然后输入到后续的网络流程中。
那么怎样实现过滤，怎样对过滤的结果做区分呢？这就是本小节和后面的ReLU、池化层所涉及的。
##### 卷积的实现
用一个图来形象的说明：

![pic](https://raw.githubusercontent.com/h312903294/MarkdownPicRep/master/convSobel.gif)

该图来源于[Convolutional Neural Networks - Basics](https://mlnotebook.github.io/post/CNN1/)

在tensorflow中，关于convolution的说明在[这里](https://www.tensorflow.org/api_guides/python/nn#Convolution)
在tensorflow中，执行卷积运算的api是[tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)。在该api中，我们需要注意的是以下几个参数：

**1. input**——相当于上面小黑屋举例中的那幅蒙娜丽莎的画——该变量是一个4阶的张量（a 4D tensor），shape表示为[batch,in_height,in_width,in_channels]。其中batch表示每次训练的时候输入一批图片的大小，一般填-1，表示运行时确定。in_heigth和in_width表示input的高和宽。in_channels表示input的通道个数，以图片举例，我们知道色彩有[三原色RGB](https://zh.wikipedia.org/wiki/%E5%8E%9F%E8%89%B2)，那么用矩阵来表示一个图片的时候，每种颜色都要对应一个矩阵，所以一张图片有三个矩阵，这里就可以把矩阵和通道等价起来。所以如果一个input数据需要多个通道（矩阵）来表示的话，我们需要通过指定in_channels的值来表示通道的个数，以保证后续各种运算的正确。

**2. filter**——相当于上面提到的手电筒——是一个4阶的张量（a 4D tensor），shape表示为[height,width,in_channels,out_channels]，其中height和width的值建议是奇数且相等，之所以选择奇数，可以参考[Quora](https://www.reddit.com/r/MachineLearning/comments/36rd91/any_reason_behind_the_fact_that_the_filter_size/)和[知乎](https://www.zhihu.com/question/51603070)。我个人的理解就是，如果使用奇数，filter会有一个中心点，这样的话filter在input上的移动会以中心点为基准，另外就是在input的边缘补零的话有一个中心点的话可以对称的补零。

**3. strides**——确定filter每次移动的步进，就是上面例子中移动手电筒的方式，是每次都和之前照射的区域覆盖一部分呢，还是相切呢等等——该变量也是一个4阶的张量(a 4D tensor)，shape表示为[1, stride, stride, 1]，Google给的解释如下：
> Must have strides[0] = strides[3] = 1. For the most common case of the same horizontal and vertices strides

至于strides[1]和strides[2]的具体大小，可以参考已有的model，然后多炼金几次，🙂

**4. padding**——这是为了解决手电筒照射到蒙娜丽莎的四条边的时候，边界外面那些没有颜料的地方怎么办的问题——它只有两个值，"SAME", "VALID"。这两个值对应的操作在[这里](https://www.tensorflow.org/api_guides/python/nn#Convolution)，一篇[csdn的blog](http://blog.csdn.net/lujiandong1/article/details/53728053)也比较形象的解释了这两个值。

通过上面的梳理，我们发现，只有filter是一个不能确定值的变量，它的值不是人为指定的。
这就是为什么Deep Learning中有一个learning的原因之一，因为filter的值的更新是通过整过网络训练过程中学习来的。

由于上面提到的filter的不确定，我们只会给filter一个初始化的操作，执行这个操作的api是[tf.truncated_normal_initializer](https://www.tensorflow.org/api_docs/python/tf/truncated_normal_initializer)。关于截断正态分布，[知乎](https://www.zhihu.com/question/49923924)的这个最高票说的很好，一句话就是在某个范围内做正态分布。为什么要正态分布，因为要加入少量的噪声来打破对称性和避免0梯度。如果是完全随机的话，我们后续无法通过梯度下降来更新filter，使用正态分布，能保证filter的初始值是在一个有限范围内，并且比较集中，当后续梯度下降的时候，按照向微分相反方向下降的规则，就可以迅速更新filter。

一般来说，我们使用tf.truncated_normal_initializer，只修改stddev这个参数，即[标准差](https://zh.wikipedia.org/wiki/%E6%A8%99%E6%BA%96%E5%B7%AE)。这就意味着，filter的值是以0为均值的一个截断正态分布。stddev的取值可以参考已有模型，我习惯取0.1。

那么卷积的操作就完了？并没有，而且后面这个操作也是非常重要的！

当我们调用完tf.nn.conv2d，需要对结果加biases（偏置项），即：
$$ Y = \sum (weight * input) + bias $$

bias是一个初始化为非常小的正数的常数——通过调用tf.constant_initializer。
增加这个偏置项的意义可以参考[stackoverfow](https://stackoverflow.com/questions/25792577/is-the-bias-node-necessary-in-very-large-neural-networks)和另一个[stackoverflow](https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks)，在这两个问题的received answer中都提到了一个单词，shift。
我个人的理解是，当过滤了一个特征出来，为了让这个特征更加的泛化（generalization），需要调整这个特征以让他适配更大范围内的差值，因此加一个bias。用线性方程$y=f(x)+b$举例，$y=f(x)+b (b>0)$对$y=f(x)$做了b大小的平移，如果令$y=f(x)+b>0$，那么相同$y$的取值范围，得到的$x$的取值范围比$y=f(x)>0$大。

至此，卷积的原理和过程都梳理完毕，它是一个CNN能工作的基石。
#### ReLU
先解释一下神经网络中激活函数（activation function）的概念。激活函数的[wiki](https://en.wikipedia.org/wiki/Activation_function)用digital network做类比，[Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)用人脑做类比。我个人总结，一句话：O不OK？（请自行脑补声音）。如果不OK，那么就放弃，如果OK，那么先留下这个卷积结果再等待后续处理。这类函数降低了整个网络的复杂度，因为他去除了一些卷积结果。如果前面的英文看起来有难度，这里有个[知乎](https://www.zhihu.com/question/22334626)上的问题还是能解决你的疑惑的。如果还不理解，那么我们用CNN模拟的对象，即人脑做说明。我们知道，人脑中的神经元是通过电信号的刺激来做出反应的，既然是电信号那么就会有电压，如果电压不够，刺激不了神经元，那么对这个电信号就会被放弃。因为你都不能刺激神经元，那对人脑来说你就是辣鸡，所以需要去其糟粕，留其精华。

ReLU，是Rectified Linear Unit的简写，中文名线性整流函数，它的[wiki](https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E6%95%B4%E6%B5%81%E5%87%BD%E6%95%B0)。
函数形式$f(x)=max(0,x)$，这个函数也有几个变种，在它的wiki中有介绍。

为什么现在很多选用ReLU做激活函数？[AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)的3.1节从工程训练方面解释了用ReLU能减少时间且保证效果，[ReLU激活函数](http://www.cnblogs.com/neopenx/p/4453161.html)从生物神经方面来解释，[知乎](https://www.zhihu.com/question/29021768)这个问题下面的回答也解释了为啥“钦定”ReLU来做激活函数。还有就是ReLU对解决梯度消失问题(vanishing gradient problem，[wiki](https://en.wikipedia.org/wiki/Vanishing_gradient_problem))的效果很好。

当然，ReLU不是适用于所有模型的。如果你的训练效果不理想，不妨从换一个激活函数入手。
### 池化层
#### pooling
pooling层也叫downsampling层，它的作用是两个：
**1. 当输入作出少量平移时，pooling能够帮助输入的表示近似不变**
**2. 保留主要特征同时减少参数和计算量，防止过拟合，提高模型泛华能力**。
具体可以参考[知乎](https://www.zhihu.com/question/36686900)，更加详细的可以看[《deep learning》](http://www.deeplearningbook.org/)的9.3和9.4节。

pooling的实现由几种方式，max pooling，average pooling和L2-norm pooling等，目前一般使用max pooling，即取pool size中的最大值。在tensorflow中的api是tf.nn.maxpool，它的参数中ksize的第二、三个参数上一层filter的宽和高。

#### local response normalization
根据[AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)这个主要是为了在数据量较大的情况下，防止神经元出现饱和——即对各种输入数据都满足，增加泛化能力。根据[What Is Local Response Normalization In Convolutional Neural Networks](https://prateekvjoshi.com/2016/04/05/what-is-local-response-normalization-in-convolutional-neural-networks/)，lrn参考了神经学上的侧面抑制(lateral inhibition)，并且配合ReLU有奇效。[Quora](https://www.quora.com/Why-would-a-saturated-neuron-be-a-problem)上的这个回答解释了什么是饱和的神经元
> We say that a neuron is saturated for these activation functions when it takes on values that are close to the boundaries of this range.

并且说明了饱和的神经元是怎样影响训练的。

但是，目前有观点说lrn没啥用，请看[知乎](https://www.zhihu.com/question/26871787)，在这个[链接](https://books.google.com/books?id=H2c9DwAAQBAJ&pg=PA300&lpg=PA300&dq=lrn++layer+++effect&source=bl&ots=FWmNM7ULqG&sig=osAUQHWKvCE6B0Mmk3qXLu_J36c&hl=zh-CN&sa=X&ved=0ahUKEwjJ7s2zyIHYAhXI1IMKHTfXCGQQ6AEIZzAH#v=onepage&q=lrn%20%20layer%20%20%20effect&f=false)中给出了移除lrn层后，精度只有小于0.1%的减少，这个和增加lrn层所带来的复杂度比起来，完全是可以接受的。看来要针对具体的模型做实验了。

### 全连接层
#### 全连接运算
全连接(fully connected)，其实从名字就可以看出这层主要是把前面的所有运算结果全部“链接”在一起。这里有一个图比较形象的表示了这个运算：

![全连接示意图](https://github.com/h312903294/MarkdownPicRep/raw/master/FCLayer%402x.png)

这一层的目的主要是对之前提取到的特征进行更高一级的提取。比如，通过之前的运算，我们得到了N段圆弧，如果要最终组成一个圆，就需要把这N段圆弧拼装到一起。
全连接层的输出是一个N维的向量，最后一层的维数是需要分辨的标签(label)的个数，因为最后会用softmax对最后一个全连接层的输出做归一化，以计算每个标签的概率，从而得到这一次训练的结果，然后同原始数据所对应的标签做比较。

在tensorflow中，tf.nn.matmul完成全连接的运算操作，然后用tf.nn.relu对matmul的结果做非线性整理。 
#### dropout
dropout的提出是在Hinton的论文[Improving neural networks by preventing
co-adaptation of feature detectors](https://arxiv.org/pdf/1207.0580.pdf)中，他用dropout解决过拟合。这篇[blog](http://www.cnblogs.com/tornadomeet/p/3258122.html)对Hinton的论文有一个简单的理解。在[AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)的4.2节也有论述。另有一篇[blog](http://blog.csdn.net/hjimce/article/details/50413257)我认为说的比较全，从原理到实现都基本涉及到了。还有一篇[blog](http://blog.csdn.net/stdcoutzyx/article/details/49022443)对论文的分析很不错，我很赞同用有性繁殖和无性繁殖来类比dropout的作用，有性繁殖之所以能适应更加复杂的环境，是因为它既能把优良基因传下去，也能让有害基因有概率的传不下去。有一张图表示了这个过程：
![dropout pic](https://github.com/h312903294/MarkdownPicRep/raw/master/dropout.png)
该图来源于[dropout](https://irenelizihui.files.wordpress.com/2016/02/394275.png)

在tensorflow中，使用tf.nn.dropout完成该操作，它的参数keep_prob一般设置0.5——来源于上面的Hinton的论文，即此次传递“丢弃”一半的全连接结果。被“丢弃”的部分并没有消失，它们的权重也保留下来了——只是暂时不更新，下次样本输入时它们可能又得工作。那么每次训练后，只有一半的结果会进入到后向传播的处理中。

总结下，前向传递包含了卷积层(convolution layer)、池化层(pooling layer)和全连接层(fully connected layer)。
通过上面的梳理，发现过拟合对于CNN的训练结果有着非常大的影响，会直接导致CNN对测试数据或者真是数据的预测出现严重问题，因此需要对过拟合进行一些针对性的预防。[How can I avoid overfitting](https://www.quora.com/How-can-I-avoid-overfitting)是Quora上的有一个相关问题，最高赞给了三个策略：
>1. collect more data
>2. use ensembling methods that “average” models
>3. choose simpler models / penalize complexity

如果能收集到更多的数据并且有与之相配的处理资源，那么第一个策略是最好的。
第三个策略更多的是和已有模型做一个对比，而且第三个策略也行会导致欠拟合——如果数据太小，且网络太简单。
那么，很明显，第二个策略对于我们来说，是一个很有性价比的策略，最高赞也是这样认为的。第二个策略实际上就是从技术上去平均(average)预防过拟合。在上面提到的Hinton的论文中也在强调要
> Another way to view the dropout procedure is as a very efficient way of performing model averaging with neural networks
>A good way to reduce the error on the test set is to
average the predictions produced by a very large number of different networks
在论文中，提到了一个观点“mean network”也是和“average models”相对应。那么为了做到average，有以下几种方法（由于笔者自己的眼界问题，不能搜集完全，敬请谅解）：
**1.** 数据集扩展(data augmentation)，
**2.** 正则化(Ragularization)包括L1、L2(L2 Regularization也叫weight decay)，
**3.** dropout

## 损失函数(loss function)
当这个过程结束后，需要对训练的结果进行评估，这个评估一般用损失函数(loss function)来做。它的[wiki](https://en.wikipedia.org/wiki/Loss_function)，[知乎](https://www.zhihu.com/question/52398145)和[blog](http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/)两个联合起来看能比较好的了解损失函数的构成，即由经验风险函数和结构风险函数组成：

$$ \theta^* = \arg \min_\theta \frac{1}{N}{}\sum_{i=1}^{N} L(y_i, f(x_i; \theta)) + \lambda\  \Phi(\theta) $$

对于分类问题，使用比较广泛的损失函数是交叉熵，[知乎](https://www.zhihu.com/question/41252833)比较好的解释了交叉熵。这里有一篇[blog](http://blog.csdn.net/marsjhao/article/details/72630147)介绍了在tensorflow中的4个交叉熵相关函数。

## 后向传递(backward pass)
按照我对[cs231n/optimization-2](http://cs231n.github.io/optimization-2/)的理解，后向传递其实就是对网络进行逆序的求导过程。
>During backward pass we then successively compute (in reverse order) the corresponding variables that hold the gradients of those variables.

## 权值更新(weight update)
用一个公式表示即：
$$ W_n = W_{n-1} - \eta \frac{\theta E}{\theta W_{n-1}} $$
其中，$\eta$是learning rate，$E$是loss function的值。
[blog](http://blog.csdn.net/happyer88/article/details/46772347)同时给出了反向传播和权值更新的公式推导，[Back Propagation Weight Update Rule](http://www.philbrierley.com/main.html?code/bpproof.html&code/codeleft.html)也做了权值更新的公式推导。

## 参考
[A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)

[LayerParams](https://code.google.com/archive/p/cuda-convnet/wikis/LayerParams.wiki)

[Must Know Tips/Tricks in Deep Neural Networks (by Xiu-Shen Wei)](http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html)

[《Deep Learning》](http://www.deeplearningbook.org/)

[Notes on Convolutional Neural Networks](http://cogprints.org/5869/1/cnn_tutorial.pdf)






